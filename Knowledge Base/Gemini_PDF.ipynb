{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMIwp7AoOAdw"
      },
      "outputs": [],
      "source": [
        "pip install google-generativeai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF-EXpcWOP33"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import google.generativeai as genai\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "def upload_to_gemini(path, mime_type=None):\n",
        "  \"\"\"Uploads the given file to Gemini.\n",
        "\n",
        "  See https://ai.google.dev/gemini-api/docs/prompting_with_media\n",
        "  \"\"\"\n",
        "  file = genai.upload_file(path, mime_type=mime_type)\n",
        "  print(f\"Uploaded file '{file.display_name}' as: {file.uri}\")\n",
        "  return file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC79krKbOkhs"
      },
      "outputs": [],
      "source": [
        "def wait_for_files_active(files):\n",
        "  \"\"\"Waits for the given files to be active.\n",
        "\n",
        "  Some files uploaded to the Gemini API need to be processed before they can be\n",
        "  used as prompt inputs. The status can be seen by querying the file's \"state\"\n",
        "  field.\n",
        "\n",
        "  This implementation uses a simple blocking polling loop. Production code\n",
        "  should probably employ a more sophisticated approach.\n",
        "  \"\"\"\n",
        "  print(\"Waiting for file processing...\")\n",
        "  for name in (file.name for file in files):\n",
        "    file = genai.get_file(name)\n",
        "    while file.state.name == \"PROCESSING\":\n",
        "      print(\".\", end=\"\", flush=True)\n",
        "      time.sleep(10)\n",
        "      file = genai.get_file(name)\n",
        "    if file.state.name != \"ACTIVE\":\n",
        "      raise Exception(f\"File {file.name} failed to process\")\n",
        "    return file\n",
        "  print(\"...all files ready\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGN6EiUGOm6J"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "generation_config = {\n",
        "  \"temperature\": 0,\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 64,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br086n8COFIf"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel(\n",
        "  model_name=\"gemini-1.5-flash\",\n",
        "  generation_config=generation_config,\n",
        "  # safety_settings = Adjust safety settings\n",
        "  # See https://ai.google.dev/gemini-api/docs/safety-settings\n",
        ")\n",
        "\n",
        "# TODO Make these files available on the local file system\n",
        "# You may need to update the file paths\n",
        "files = [\n",
        "  upload_to_gemini(\"2407.01449v2.pdf\", mime_type=\"application/pdf\"),\n",
        "]\n",
        "\n",
        "# Some files have a processing delay. Wait for them to be ready.\n",
        "wait_for_files_active(files)\n",
        "\n",
        "chat_session = model.start_chat(\n",
        "  history=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "        files[0],\n",
        "        \"What is the title of this paper?\\n\\n\",\n",
        "      ],\n",
        "    },\n",
        "\n",
        "  ]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8MkRlFYOFPK"
      },
      "outputs": [],
      "source": [
        "response = chat_session.send_message(\"How many Figures are in the paper?\")\n",
        "\n",
        "print(response.text)\n",
        "\n",
        "print(response.usage_metadata)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPFiyd1cPGUh"
      },
      "outputs": [],
      "source": [
        "response = chat_session.send_message(\"Who are the authors?\")\n",
        "\n",
        "print(response.text)\n",
        "print(response.usage_metadata)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG-iEcL4PI14"
      },
      "outputs": [],
      "source": [
        "response = chat_session.send_message(\"What are the major contributions of the paper accordig to the authors?\")\n",
        "\n",
        "print(response.text)\n",
        "\n",
        "print(response.usage_metadata)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-XTWoEdPTeX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate text from text-and-image input\n",
        "\n",
        "The Gemini API supports multimodal inputs that combine text with media files. The following example shows how to generate text from text-and-image input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import PIL.Image\n",
        "\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "organ = PIL.Image.open(media / \"organ.jpg\")\n",
        "response = model.generate_content([\"Tell me about this instrument\", organ])\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate JSON\n",
        "When the model is configured to output JSON, it responds to any prompt with JSON-formatted output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "genai.configure(api_key=os.environ[\"API_KEY\"])\n",
        "\n",
        "# Using `response_mime_type` requires either a Gemini 1.5 Pro or 1.5 Flash model\n",
        "model = genai.GenerativeModel('gemini-1.5-flash',\n",
        "                              # Set the `response_mime_type` to output JSON\n",
        "                              generation_config={\"response_mime_type\": \"application/json\"})\n",
        "\n",
        "prompt = \"\"\"\n",
        "  List 5 popular cookie recipes.\n",
        "  Using this JSON schema:\n",
        "    Recipe = {\"recipe_name\": str}\n",
        "  Return a `list[Recipe]`\n",
        "  \"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCpZJS6AH--B"
      },
      "source": [
        "### Adding Context Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVMmD28mIA7w"
      },
      "outputs": [],
      "source": [
        "files = [\n",
        "  upload_to_gemini(\"2407.01449v2.pdf\", mime_type=\"application/pdf\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dyZZY1FIcLT"
      },
      "outputs": [],
      "source": [
        "path_to_pdf_file = '2403.06634.pdf'\n",
        "\n",
        "# Upload the video using the Files API\n",
        "pdf_file = genai.upload_file(path=path_to_pdf_file)\n",
        "\n",
        "# Wait for the file to finish processing\n",
        "while pdf_file.state.name == 'PROCESSING':\n",
        "  print('Waiting for video to be processed.')\n",
        "  time.sleep(2)\n",
        "  pdf_file = genai.get_file(pdf_file.name)\n",
        "\n",
        "print(f'Video processing complete: {pdf_file.uri}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym5nDUNFJPbK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.generativeai import caching\n",
        "import datetime\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1Qv5RqGJDqA"
      },
      "outputs": [],
      "source": [
        "# Create a cache with a 5 minute TTL\n",
        "cache = caching.CachedContent.create(\n",
        "    model='models/gemini-1.5-flash-001',\n",
        "    display_name='PDF-file', # used to identify the cache\n",
        "    system_instruction=(\n",
        "        'You are an expert PDF file analyzer, and your job is to answer '\n",
        "        'the user\\'s query based on the PDF file you have access to.'\n",
        "    ),\n",
        "    contents=[pdf_file],\n",
        "    ttl=datetime.timedelta(minutes=15),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Mfw0joBJQ7L"
      },
      "outputs": [],
      "source": [
        "# Construct a GenerativeModel which uses the created cache.\n",
        "model = genai.GenerativeModel.from_cached_content(cached_content=cache)\n",
        "\n",
        "# Query the model\n",
        "response = model.generate_content([(\n",
        "    'What is the title of the paper?'\n",
        "    'Who are the authors? '\n",
        "    'What are the major contributions of the paper accordig to the authors?'\n",
        "    'they were introduced for the first time.'\n",
        ")])\n",
        "\n",
        "print(response.usage_metadata)\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xVp4b6kMhXu"
      },
      "outputs": [],
      "source": [
        "# Construct a GenerativeModel which uses the created cache.\n",
        "model = genai.GenerativeModel.from_cached_content(cached_content=cache)\n",
        "\n",
        "# Query the model\n",
        "response = model.generate_content([(\n",
        "    'What is the title of the paper?'\n",
        "    'Who are the authors? provide a list '\n",
        "    'What are the major contributions of the paper accordig to the authors?'\n",
        "    'they were introduced for the first time.'\n",
        ")])\n",
        "\n",
        "print(response.usage_metadata)\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5Ec4WToKqGL"
      },
      "outputs": [],
      "source": [
        "for c in caching.CachedContent.list():\n",
        "  print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV_zK0l5NLOJ"
      },
      "outputs": [],
      "source": [
        "response = model.generate_content(\"What is the main theme of the paper?\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_ca2h1aPAXB"
      },
      "outputs": [],
      "source": [
        "print(response.usage_metadata)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs0j1L7wPS5_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
